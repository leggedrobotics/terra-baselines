train_config:
  train_type: "PPO"
  num_train_steps: 3000000000
  evaluate_every_epochs: 4000

  env_name: "Terra"
  env_kwargs: {}
  env_params: {}
  num_test_rollouts: 128
  # num_steps_test_rollouts: 600  # TODO make it work (tracer problem)

  use_action_masking: False
  mask_out_arm_extension: True

  fast_compile: True  # if True, uses jax.lax.scan to iterate over the minibatches
  num_train_envs: 3000  # 5120  # Number of parallel env workers
  epoch_ppo: 1  # "Number of PPO epochs on a single batch"
  n_steps: 16 # "GAE n-steps"
  n_minibatch: 16 # "Number of PPO minibatches" -> Note: compile time scales with it!
  lr_begin: 3e-04  # Start PPO learning rate
  lr_end: 3e-04 #  End PPO learning rate
  lr_warmup: 0.05 # Prop epochs until warmup is completed 
  max_grad_norm: 0.5  # Global norm to clip gradients by
  gamma: 0.995  # Discount factor
  clip_eps: 0.2 # "Clipping range"
  gae_lambda: 0.95 # "GAE lambda"
  entropy_coeff: 0.01 # "Entropy loss coefficient"
  critic_coeff: 0.5  # "Value loss coefficient"

  # Rewards
  max_reward: 200  # TODO take it from env config (used for rewards normalization)

  # Model
  clip_action_maps: True  # clips the action maps to [-1, 1]
  maps_net_normalization_bounds: [-1, 8]  # automatically set to [-1, 1] if clip_action_maps is True
  local_map_normalization_bounds: [-16, 16]
  loaded_max: 100

  # Curriculum
  # increase_dof_threshold: 0.01  # value_losses_individual / targets_individual < increase_dof_threshold
  increase_dof_consecutive_episodes: 1  # if the agent can solve X consecutive episodes on the given env, increase the dof
  max_episodes_no_dones: 10  # number of episodes per env with no dones before decreasing the dof
  max_increase_dof_ratio: 0.3  # max percentage of configs that can increase dof in one step
  max_decrease_dof_ratio: 0.3  # max percentage of configs that can decrease dof in one step
  random_dof_ratio: 0.0  # % of envs that get a random dofs sampled from the ones already unlocked
  last_dof_type: 'none'  # ['none', 'random', 'sparse', 'random_from_selection']

  # Reset manager
  activate_reset_manager: False
  context_length: 6  # store N steps in the past to evaluate if force reset

  network_name: "SimplifiedCoupledCategoricalNet" # "SimplifiedDecoupledCategoricalNet"

  n_evals_save: 9
  profile: False

log_config:
  time_to_track: ["num_steps"]
  what_to_track: ["return"]
  verbose: false
  print_every_k_updates: 1
  overwrite: 1
  model_type: "jax"
